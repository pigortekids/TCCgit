{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tcc_2020.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuI44SCnhXAh",
        "colab_type": "code",
        "outputId": "0186e73d-d110-41aa-9d2e-5670700818d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "caminho_drive = '/content/drive/My Drive/colab/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7fryClKhoEg",
        "colab_type": "code",
        "outputId": "f4f6c4c7-be1a-44bc-b6ab-f877e3878ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "!pip install keras-rl2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
            "Requirement already satisfied: tensorflow==2.0.0-beta1 in /usr/local/lib/python3.6/dist-packages (from keras-rl2) (2.0.0b1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1->keras-rl2) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1->keras-rl2) (1.28.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1->keras-rl2) (0.9.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1->keras-rl2) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1->keras-rl2) (0.8.1)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1->keras-rl2) (1.14.0.dev2019060501)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1->keras-rl2) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1->keras-rl2) (3.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1->keras-rl2) (1.18.3)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1->keras-rl2) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1->keras-rl2) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1->keras-rl2) (1.14.0a20190603)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-beta1->keras-rl2) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-beta1->keras-rl2) (46.1.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1->keras-rl2) (3.2.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu7BV3dWlZUQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "8a9a1a02-b34d-4ea7-ef59-e581b0f80114"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import GreedyQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy\n",
        "from keras.callbacks.callbacks import LambdaCallback\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A71Tu540ho0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arquivo = pd.read_csv( caminho_drive + 'consolidado.csv' )\n",
        "colunas = arquivo.columns\n",
        "val_arquivo = arquivo[['preco', 'preco_pon', 'qnt_soma', 'max', 'min', 'IND', 'ISP']]\n",
        "datas_pd = arquivo[['dt', 'hr_int']]\n",
        "datas_np = arquivo.rename_axis('ID').values\n",
        "indexes_primeiro_do_dia = datas_pd[ datas_pd['hr_int'] == 32700000 ].index\n",
        "scaler = StandardScaler().fit( val_arquivo )\n",
        "valores_norm = scaler.transform( val_arquivo )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUdWWtzApftq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cria_modelo(n_acoes, input_shape, porc_dropout):\n",
        "\n",
        "    rna = models.Sequential()\n",
        "    rna.add(layers.Flatten(input_shape=input_shape))\n",
        "\n",
        "    rna.add(layers.Dense(512,activation='relu', kernel_constraint=max_norm(3)))\n",
        "    rna.add(layers.Dropout(porc_dropout))\n",
        "\n",
        "    rna.add(layers.Dense(256,activation='relu', kernel_constraint=max_norm(3)))\n",
        "    rna.add(layers.Dropout(porc_dropout))\n",
        "\n",
        "    rna.add(layers.Dense(128,activation='relu', kernel_constraint=max_norm(3)))\n",
        "    rna.add(layers.Dropout(porc_dropout))\n",
        "\n",
        "    rna.add(layers.Dense(64,activation='relu', kernel_constraint=max_norm(3)))\n",
        "    rna.add(layers.Dropout(porc_dropout))\n",
        "\n",
        "    rna.add(layers.Dense(n_acoes, activation='softmax'))\n",
        "\n",
        "    return rna"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWHQOUkTpnvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TradeEnv(gym.Env):\n",
        "    def __init__(self, n_parametros, input_shape, max_val, min_val, val_arquivo):\n",
        "        self.n_acoes = 3\n",
        "        self.n_parametros = n_parametros\n",
        "        self.action_space = spaces.Discrete(self.n_acoes)\n",
        "        self.observation_space = spaces.Box( low=min_val , high=max_val , shape=input_shape )\n",
        "        self.n_contratos = None\n",
        "        self.n_contratos_ant = None\n",
        "        self.n_max_contratos = 1\n",
        "        self.val_carteira = None\n",
        "        self.val_compra = None\n",
        "        self.val_compra_norm = None\n",
        "        self.step_arquivo = None\n",
        "        self.valores = val_arquivo\n",
        "        self.step_ini = None\n",
        "        self.step_fim = None\n",
        "        self.reset()\n",
        "\n",
        "    def observacao(self):\n",
        "        obs = []\n",
        "\n",
        "        for valor in self.valores[ self.step_arquivo ]:\n",
        "            obs.append( valor )\n",
        "\n",
        "        return obs\n",
        "\n",
        "    def step(self, acao):\n",
        "        self.n_contratos_ant = self.n_contratos\n",
        "        acabou = None\n",
        "        recompensa = 0\n",
        "        valor = 0\n",
        "        valor_norm = 0\n",
        "        valor_ant = 0\n",
        "        valor_ant_norm = 0\n",
        "        valor_dif_norm = 0\n",
        "\n",
        "        if self.step_arquivo + 1 == self.step_fim:\n",
        "            acabou = True\n",
        "            if self.n_contratos > 0:\n",
        "                self.n_contratos = 0\n",
        "                recompensa = valor_norm - self.val_compra_norm #diferenca entre valor de compra e\n",
        "                self.val_carteira += self.val_carteira * ((valor - self.val_compra) / self.val_compra)\n",
        "\n",
        "        else:\n",
        "            acabou = False\n",
        "\n",
        "            val_linha_norm = self.valores[ self.step_arquivo ]\n",
        "            val_linha = scaler.inverse_transform( val_linha_norm )\n",
        "            valor_norm = val_linha_norm[0]\n",
        "            valor = val_linha[0]\n",
        "\n",
        "            val_linha_ant_norm = self.valores[ self.step_arquivo - 1 ]\n",
        "            val_linha_ant = scaler.inverse_transform( val_linha_ant_norm )\n",
        "            valor_ant_norm = val_linha_ant_norm[0]\n",
        "            valor_ant = val_linha_ant[0]\n",
        "            if self.step_arquivo - 1 < self.step_ini:\n",
        "                valor_ant_norm = valor_norm\n",
        "                valor_ant = valor\n",
        "\n",
        "            valor_dif_norm = valor_norm - valor_ant_norm #diferenca de preco\n",
        "\n",
        "            if acao == 1: #compra\n",
        "                if self.n_contratos < self.n_max_contratos:\n",
        "                    self.val_compra_norm = valor_norm\n",
        "                    self.val_compra = valor\n",
        "                    self.n_contratos += 1\n",
        "                    #recompensa = -valor_dif #comprou quando subiu ruim, comprou quando desceu bom\n",
        "            elif acao == 2: #vende\n",
        "                if self.n_contratos > 0:\n",
        "                    self.n_contratos -= 1\n",
        "                    recompensa = valor_norm - self.val_compra_norm #diferenca entre valor de compra e\n",
        "                    self.val_carteira += self.val_carteira * ((valor - self.val_compra) / self.val_compra)\n",
        "            #elif acao == 0: #nao faz nada\n",
        "            #    if self.n_contratos > 0:\n",
        "            #        recompensa = valor_dif #caso tenha contrato, o ganho é a diferenca dos precos\n",
        "            #    else:\n",
        "            #        recompensa = -valor_dif #caso nao tenha contrato, o ganho é o negativo da diferenca dos precos\n",
        "\n",
        "            self.step_arquivo += 1\n",
        "\n",
        "        dicionario = {'carteira':self.val_carteira}\n",
        "        dicionario['n_contratos'] = self.n_contratos\n",
        "        dicionario['valor_dif'] = valor_dif_norm\n",
        "        dicionario['valor'] = valor\n",
        "        dicionario['val_compra'] = self.val_compra\n",
        "\n",
        "        if self.val_compra == 0:\n",
        "            dicionario['val_dif_porc'] = 0\n",
        "        else:\n",
        "            dicionario['val_dif_porc'] = valor / self.val_compra\n",
        "        return self.observacao(), recompensa, acabou, dicionario\n",
        "\n",
        "    def reset(self):\n",
        "        self.n_contratos = 0\n",
        "        self.n_contratos_ant = 0\n",
        "        self.val_carteira = 100\n",
        "        index_dia = np.random.choice( len(indexes_primeiro_do_dia) )\n",
        "        self.step_arquivo = indexes_primeiro_do_dia[ index_dia ]\n",
        "        self.step_ini = self.step_arquivo\n",
        "        if index_dia + 1 == len(indexes_primeiro_do_dia):\n",
        "            self.step_fim = len(self.valores)\n",
        "        else:\n",
        "            self.step_fim = indexes_primeiro_do_dia[ index_dia + 1 ]\n",
        "        self.val_compra = 0\n",
        "        self.val_compra_norm = 0\n",
        "        return self.observacao()\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print( '{}'.format( self.val_carteira ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxQz1a9Rp1yT",
        "colab_type": "code",
        "outputId": "8611a2a2-b4f1-43c3-c444-2cf2a43674cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "np.random.seed(10)\n",
        "\n",
        "n_parametros = len(valores_norm[0]) #define o numero de entradas da rede\n",
        "janela_aprendizado = 10 #quantas linhas da memoria vai pegar pra treinar o modelo por vez\n",
        "input_shape = ( janela_aprendizado, n_parametros, )\n",
        "max_val_norm = np.amax( valores_norm )\n",
        "min_val_norm = np.amin( valores_norm )\n",
        "ambiente = TradeEnv( n_parametros , input_shape , max_val_norm , min_val_norm , valores_norm ) #cria o ambiente para o jogo\n",
        "n_acoes = ambiente.n_acoes #define o numero de saidas da rede\n",
        "\n",
        "n_steps_1 = 300000\n",
        "n_steps_2 = 300000\n",
        "n_steps_aquecimento = 100\n",
        "\n",
        "memoria = SequentialMemory(limit=50000, window_length=janela_aprendizado)\n",
        "\n",
        "modelo = cria_modelo( 3, input_shape, 0.3 )\n",
        "metricas = ['accuracy']\n",
        "\n",
        "n_steps_1_por_1000 = int(n_steps_1 / 1000)\n",
        "n_steps_2_por_1000 = int(n_steps_2 / 1000)\n",
        "agora = str( datetime.now() - timedelta(hours = 3) ).replace(' ', '_') #horario de brasilia UTC-3\n",
        "caminho_export = '{0}{1}_{2}k_{3}k_steps_'.format( caminho_drive, agora, n_steps_1_por_1000, n_steps_2_por_1000 )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1Ye9Jomp6OK",
        "colab_type": "code",
        "outputId": "819bc1a1-8eca-42f4-97ba-596cd4d71e40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "eps_max = 1\n",
        "eps_min = 0\n",
        "politica = LinearAnnealedPolicy( inner_policy=EpsGreedyQPolicy(), attr='eps', value_max=eps_max, value_min=eps_min, nb_steps=n_steps_1, value_test=0 ) #0.2 antes do decay\n",
        "agente = DQNAgent( model=modelo, policy=politica, nb_actions=n_acoes, memory=memoria, enable_double_dqn=True, test_policy=GreedyQPolicy(), nb_steps_warmup=n_steps_aquecimento )\n",
        "agente.compile(optimizers.Adam(), metrics=metricas)\n",
        "\n",
        "resultados_treino = []\n",
        "intervalo_log_1 = int( n_steps_1 / 5 )\n",
        "intervalo_modelo_1 = int( n_steps_1 / 5 )\n",
        "callbacks_treino_1 = []\n",
        "callbacks_treino_1.append( FileLogger( 'log_1.log' ) )\n",
        "callbacks_treino_1.append( FileLogger( '{0}log_1.log'.format( caminho_export ) ) )\n",
        "callbacks_treino_1.append( LambdaCallback( on_batch_end=lambda batch,logs: resultados_treino.append( logs ) ) )\n",
        "callbacks_treino_1.append( ModelIntervalCheckpoint( '{0}dqn_1.h5f'.format( caminho_export ) , interval=intervalo_modelo_1 ) )\n",
        "historia_treino = agente.fit(env=ambiente, nb_steps=n_steps_1, visualize=False, verbose=1, callbacks=callbacks_treino_1, log_interval=intervalo_log_1)\n",
        "#historia_treino = agente.fit(env=ambiente, nb_steps=n_steps_1, visualize=False, verbose=0, callbacks=callbacks_treino_1)\n",
        "\n",
        "agente.save_weights('dqn_1.h5f', overwrite=True)\n",
        "agente.save_weights( '{0}dqn_1.h5f'.format( caminho_export ), overwrite=True )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 300000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "60000/60000 [==============================] - 1217s 20ms/step - reward: 5.6468e-05\n",
            "566 episodes - episode_reward: 0.006 [-2.763, 1.719] - loss: 0.004 - accuracy: 0.335 - mean_q: 0.336 - mean_eps: 0.900 - carteira: 99.518 - n_contratos: 0.493 - valor_dif: -0.000 - valor: 3838.270 - val_compra: 3794.949 - val_dif_porc: 0.970\n",
            "\n",
            "Interval 2 (60000 steps performed)\n",
            "60000/60000 [==============================] - 1238s 21ms/step - reward: -1.9508e-04\n",
            "566 episodes - episode_reward: -0.021 [-2.633, 1.868] - loss: 0.003 - accuracy: 0.333 - mean_q: 0.335 - mean_eps: 0.700 - carteira: 99.551 - n_contratos: 0.490 - valor_dif: 0.000 - valor: 3835.841 - val_compra: 3783.193 - val_dif_porc: 0.968\n",
            "\n",
            "Interval 3 (120000 steps performed)\n",
            "60000/60000 [==============================] - 1281s 21ms/step - reward: -1.2492e-04\n",
            "566 episodes - episode_reward: -0.013 [-2.426, 1.852] - loss: 0.003 - accuracy: 0.334 - mean_q: 0.335 - mean_eps: 0.500 - carteira: 99.548 - n_contratos: 0.485 - valor_dif: 0.000 - valor: 3841.636 - val_compra: 3756.049 - val_dif_porc: 0.959\n",
            "\n",
            "Interval 4 (180000 steps performed)\n",
            "60000/60000 [==============================] - 1263s 21ms/step - reward: 1.1006e-04\n",
            "566 episodes - episode_reward: 0.012 [-2.373, 1.680] - loss: 0.003 - accuracy: 0.333 - mean_q: 0.335 - mean_eps: 0.300 - carteira: 99.552 - n_contratos: 0.489 - valor_dif: 0.000 - valor: 3836.630 - val_compra: 3674.512 - val_dif_porc: 0.939\n",
            "\n",
            "Interval 5 (240000 steps performed)\n",
            "60000/60000 [==============================] - 1327s 22ms/step - reward: 2.4213e-04\n",
            "done, took 6327.331 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVdnXx4FXLhz",
        "colab_type": "code",
        "outputId": "f07b4f2f-ea74-4f68-e094-77d6bf121ee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "agente = DQNAgent( model=modelo, policy=GreedyQPolicy(), nb_actions=n_acoes, memory=memoria )\n",
        "agente.compile(optimizers.Adam(), metrics=metricas)\n",
        "agente.load_weights('dqn_1.h5f')\n",
        "\n",
        "intervalo_log_2 = int( n_steps_2 / 5 )\n",
        "intervalo_modelo_2 = int( n_steps_2 / 5 )\n",
        "callbacks_treino_2 = []\n",
        "callbacks_treino_2.append( FileLogger( 'log_2.log' ) )\n",
        "callbacks_treino_2.append( FileLogger( '{0}log_2.log'.format( caminho_export ) ) )\n",
        "callbacks_treino_2.append( LambdaCallback( on_batch_end=lambda batch,logs: resultados_treino.append( logs ) ) )\n",
        "callbacks_treino_2.append( ModelIntervalCheckpoint( '{0}dqn_2.h5f'.format( caminho_export ) , interval=intervalo_modelo_2 ) )\n",
        "historia_treino_2 = agente.fit(env=ambiente, nb_steps=n_steps_2, visualize=False, verbose=1, callbacks=callbacks_treino_2, log_interval=intervalo_log_2)\n",
        "#historia_treino_2 = agente.fit(env=ambiente, nb_steps=n_steps_teste, visualize=False, verbose=0, callbacks=callbacks_treino_2)\n",
        "\n",
        "agente.save_weights('dqn_2.h5f', overwrite=True)\n",
        "agente.save_weights( '{0}dqn_2.h5f'.format( caminho_export ) , overwrite=True )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 300000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "60000/60000 [==============================] - 1204s 20ms/step - reward: 4.1560e-04\n",
            "566 episodes - episode_reward: 0.044 [-2.507, 2.227] - loss: 0.003 - accuracy: 0.339 - mean_q: 0.335 - carteira: 99.511 - n_contratos: 0.480 - valor_dif: 0.000 - valor: 3832.164 - val_compra: 2444.056 - val_dif_porc: 0.625\n",
            "\n",
            "Interval 2 (60000 steps performed)\n",
            "60000/60000 [==============================] - 1214s 20ms/step - reward: -1.9956e-04\n",
            "566 episodes - episode_reward: -0.021 [-2.400, 1.833] - loss: 0.003 - accuracy: 0.333 - mean_q: 0.335 - carteira: 99.554 - n_contratos: 0.404 - valor_dif: 0.000 - valor: 3847.290 - val_compra: 2238.854 - val_dif_porc: 0.570\n",
            "\n",
            "Interval 3 (120000 steps performed)\n",
            "57360/60000 [===========================>..] - ETA: 54s - reward: -4.5010e-04"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIpcGCcaHNOu",
        "colab_type": "code",
        "outputId": "dd184cf5-ade5-47d9-baa1-0ad31e42823e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "qnt_val = 0\n",
        "for log in resultados_treino:\n",
        "    if not math.isnan( log['metrics'][0] ):\n",
        "        print('valor = {:.4f}, valor_ant = {:.4f}, acao = {}, episodio = {}, n_contratos = {}, carteira = {:.4f}, valor_dif = {:.4f}'.format(log['info']['valor_norm'], log['info']['valor_ant_norm'], log['action'], log['episode'], log['info']['n_contratos'], log['info']['carteira'], log['info']['valor_dif']))\n",
        "        qnt_val += 1\n",
        "    if qnt_val > 10:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "valor = -0.9050, valor_ant = -0.9164, acao = 1, episodio = 0, n_contratos = 1, carteira = 99.4703, valor_dif = 0.0115\n",
            "valor = -0.9355, valor_ant = -0.9050, acao = 0, episodio = 0, n_contratos = 1, carteira = 99.4703, valor_dif = -0.0306\n",
            "valor = -0.9202, valor_ant = -0.9355, acao = 0, episodio = 0, n_contratos = 1, carteira = 99.4703, valor_dif = 0.0153\n",
            "valor = -0.9088, valor_ant = -0.9202, acao = 0, episodio = 0, n_contratos = 1, carteira = 99.4703, valor_dif = 0.0115\n",
            "valor = 0.0000, valor_ant = 0.0000, acao = 1, episodio = 0, n_contratos = 0, carteira = 99.4703, valor_dif = 0.0000\n",
            "valor = 0.0921, valor_ant = 0.0921, acao = 2, episodio = 1, n_contratos = 0, carteira = 100.0000, valor_dif = 0.0000\n",
            "valor = 0.1074, valor_ant = 0.0921, acao = 0, episodio = 1, n_contratos = 0, carteira = 100.0000, valor_dif = 0.0153\n",
            "valor = 0.1456, valor_ant = 0.1074, acao = 0, episodio = 1, n_contratos = 0, carteira = 100.0000, valor_dif = 0.0382\n",
            "valor = 0.1113, valor_ant = 0.1456, acao = 1, episodio = 1, n_contratos = 1, carteira = 100.0000, valor_dif = -0.0344\n",
            "valor = 0.0463, valor_ant = 0.1113, acao = 0, episodio = 1, n_contratos = 1, carteira = 100.0000, valor_dif = -0.0649\n",
            "valor = 0.0616, valor_ant = 0.0463, acao = 1, episodio = 1, n_contratos = 1, carteira = 100.0000, valor_dif = 0.0153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp6D4nVvNlWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.style.use('seaborn')\n",
        "#nm_metricas = ['recompensa', 'regret', 'loss', 'accuracy', 'mean_q', 'mean_eps']\n",
        "#nm_metricas = ['recompensa', 'loss', 'accuracy', 'mean_q', 'mean_eps']\n",
        "nm_metricas = ['recompensa', 'mean_eps', '', '', 'mean_q']\n",
        "metrica_max = [True, False, True, True, False]\n",
        "divisores = [1, 10]\n",
        "result = resultados_treino\n",
        "n_resultados = len(result)\n",
        "tipo = 'treino_full'\n",
        "pular_n_steps = n_steps_aquecimento\n",
        "fig_size_x = 20\n",
        "fig_size_y = len(nm_metricas) * 6\n",
        "\n",
        "fig, axs = plt.subplots(len(nm_metricas), len(divisores), gridspec_kw={'hspace': 0.4, 'wspace': 0.2}, figsize=( fig_size_x , fig_size_y ))\n",
        "#fig.suptitle('Medias por episodios do {}'.format(tipo))\n",
        "for metrica_i in range(len(nm_metricas)):\n",
        "\n",
        "    nm_metrica = nm_metricas[metrica_i]\n",
        "    if len(divisores) == 1:\n",
        "        axs[metrica_i].set(ylabel=nm_metrica)\n",
        "    else:\n",
        "        axs[metrica_i][0].set(ylabel=nm_metrica)\n",
        "    grafico = 0\n",
        "\n",
        "    for divisor in divisores:\n",
        "\n",
        "        axes = None\n",
        "        if len(divisores) == 1:\n",
        "            axes = axs[metrica_i]\n",
        "        else:\n",
        "            axes = axs[metrica_i][grafico]\n",
        "        grafico = 0\n",
        "        axes.set(title='Media do(a) {} a cada {} episodios'.format(nm_metrica, divisor))\n",
        "\n",
        "        hist_metricas = []\n",
        "        metricas_media = []\n",
        "        metricas_ref = []\n",
        "        ref = result[pular_n_steps]['episode']\n",
        "\n",
        "        # [ posicao em x do maior ponto na media , posicao y do maior ponto na media , maior ponto ]\n",
        "        max_metrica_media_1 = [None, None]\n",
        "        min_metrica_media_1 = [None, None]\n",
        "\n",
        "        for i in range(0, n_resultados + 1):\n",
        "            if len(hist_metricas) % divisor == 0 and len(hist_metricas) > 0:\n",
        "                metricas_media.append( sum(hist_metricas[-divisor:]) / divisor )\n",
        "                #pega maximos e minimos\n",
        "                if max_metrica_media_1[1] == None or metricas_media[-1] >= max_metrica_media_1[1]:\n",
        "                    max_metrica_media_1[0] = i / divisor\n",
        "                    max_metrica_media_1[1] = metricas_media[-1]\n",
        "                if min_metrica_media_1[1] == None or metricas_media[-1] <= min_metrica_media_1[1]:\n",
        "                    min_metrica_media_1[0] = i / divisor\n",
        "                    min_metrica_media_1[1] = metricas_media[-1]\n",
        "            if (i == n_resultados or result[i]['episode'] != ref) and len(metricas_ref) > 0:\n",
        "                hist_metricas.append( sum(metricas_ref) / len(metricas_ref) )\n",
        "                ref += 1\n",
        "                metricas_ref = []\n",
        "            if i < n_resultados:\n",
        "                if metrica_i == 0: #recompensa\n",
        "                    metricas_ref.append( result[i]['reward'] )\n",
        "                elif metrica_i == 1: #regret\n",
        "                    metricas_ref.append( result[i]['info']['carteira'] )\n",
        "                else: #outras metricas\n",
        "                    if metrica_i - 1 == len(result[i]['metrics']): #se for o epsilon\n",
        "                        metricas_ref.append( 0 )\n",
        "                    else:\n",
        "                        if not math.isnan( result[i]['metrics'][metrica_i-1] ): #tem que ser diferente de Not a Number (NaN)\n",
        "                            metricas_ref.append( result[i]['metrics'][metrica_i-1] )\n",
        "\n",
        "        n_steps_media_treino_1 = int((n_steps_1 - pular_n_steps) / divisor)\n",
        "        n_steps_media_total = len(metricas_media)\n",
        "        #plota grafico\n",
        "        x_plot = np.arange(n_steps_media_total)\n",
        "        axes.set(xlabel='episodio / {}'.format(divisor))\n",
        "        axes.plot(x_plot, metricas_media)\n",
        "        #linha de tendencia antes do epsilon = 0\n",
        "        z = np.polyfit(x_plot[:n_steps_media_treino_1], metricas_media[:n_steps_media_treino_1], 1)\n",
        "        p = np.poly1d(z)\n",
        "        axes.plot(x_plot[:n_steps_media_treino_1], p(x_plot[:n_steps_media_treino_1]), c='#ff0000', ls=\"--\", linewidth=3, label='tendencia eps decay')\n",
        "        #linha de tendencia antes do epsilon = 0\n",
        "        z = np.polyfit(x_plot[n_steps_media_treino_1:], metricas_media[n_steps_media_treino_1:], 1)\n",
        "        p = np.poly1d(z)\n",
        "        axes.plot(x_plot[n_steps_media_treino_1:], p(x_plot[n_steps_media_treino_1:]), c='#ff0000', ls=\"--\", linewidth=3, label='tendencia eps = 0')\n",
        "        #maximo e minimo\n",
        "        if metrica_max[metrica_i]:\n",
        "            #linha de maximo_1\n",
        "            anotacao_x_max = max_metrica_media_1[0]\n",
        "            if max_metrica_media_1[0] > n_steps_media_total / 2:\n",
        "                anotacao_x_max = n_steps_media_total * 0.9\n",
        "            anotacao_y_max = ((max_metrica_media_1[1] - min_metrica_media_1[1]) * 0.6) + min_metrica_media_1[1]\n",
        "            axes.axhline(max_metrica_media_1[1], c='#ff6600', ls='--', linewidth=2, label='maximo_1')\n",
        "            axes.annotate( 'max_1 = {:.4f}'.format(max_metrica_media_1[1]), \n",
        "                                            xy=( max_metrica_media_1[0] , max_metrica_media_1[1] ),\n",
        "                                            xytext=( anotacao_x_max , anotacao_y_max ),\n",
        "                                            arrowprops=dict(facecolor='black', shrink=0.05) )\n",
        "        else:\n",
        "            #linha de minimo_1\n",
        "            anotacao_x_min = min_metrica_media_1[0]\n",
        "            if min_metrica_media_1[0] > n_steps_media_total / 2:\n",
        "                anotacao_x_min = n_steps_media_total * 0.9\n",
        "            anotacao_y_min = ((max_metrica_media_1[1] - min_metrica_media_1[1]) * 0.4) + min_metrica_media_1[1]\n",
        "            axes.axhline(min_metrica_media_1[1], c='#ff6600', ls='--', linewidth=2, label='minimo_1')\n",
        "            axes.annotate( 'min_1 = {:.4f}'.format(min_metrica_media_1[1]),\n",
        "                                            xy=( min_metrica_media_1[0] , min_metrica_media_1[1] ),\n",
        "                                            xytext=( anotacao_x_min , anotacao_y_min ),\n",
        "                                            arrowprops=dict(facecolor='black', shrink=0.03) )\n",
        "        #linha do epsilon = 0\n",
        "        axes.axvline( x=n_steps_media_treino_1, c='#00ff00' , linewidth=1, label='epsilon = 0')\n",
        "        \n",
        "        grafico += 1\n",
        "\n",
        "fig.show()\n",
        "fig.savefig('result_{}.png'.format(tipo), bbox_inches='tight') #salva o grafico em uma foto\n",
        "fig.savefig('{0}result_{1}.png'.format( caminho_export, tipo ), bbox_inches='tight') #salva o grafico em uma foto"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR-vXVGPWXvj",
        "colab_type": "code",
        "outputId": "b95e0127-b040-460f-8515-480e8c14e032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "retorno = agente.test(ambiente, nb_episodes=1, verbose=1, visualize=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 1 episodes ...\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "Episode 1: reward: 0.000, steps: 106\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}