{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#################################### IMPORTS ###################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from keras.backend.tensorflow_backend import set_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### INICIALIZACAO DE VARIAVEIS ################################################\n",
    "index_arquivo = ['preco', 'hr_int']#, 'preco_pon', 'qnt_soma', 'max', 'min', 'IND', 'ISP'] #index do arquivo\n",
    "steps = [] # 9h04 -> 17h50 a cada 5 segundos \n",
    "epocas = 1000 #quantidade de vezes que vai rodar todos os dias\n",
    "janela = 10 #janela de valores\n",
    "n_variaveis = len(index_arquivo) #'preco', 'hr_int', 'preco_pon', 'qnt_soma', 'max', 'min', 'IND', 'ISP'\n",
    "l = n_variaveis-1\n",
    "n_mercado = n_variaveis*janela #inputs\n",
    "n_estados = 2 #ncont e valor\n",
    "n_neuronios = 216 #numero de neuronios da camada escondida\n",
    "lim_cont = 5\n",
    "n_saidas = 2*lim_cont+1   #numero de saidas da rede (compra, vende, segura)\n",
    "custo = 1.06/2 #custo da operao\n",
    "melhor_reward = 0\n",
    "\n",
    "versao_arquivo = 2\n",
    "\n",
    "carregar_pesos = False\n",
    "carregar_epoca_epsilon = False\n",
    "epoca_init = 0\n",
    "if carregar_epoca_epsilon:\n",
    "    file = open(\"./epoca_epsilon.txt\", \"r\")\n",
    "    valores = file.read().split(',')\n",
    "    epoca_init = int(valores[0])\n",
    "    epsilon = float(valores[1])\n",
    "    file.close()\n",
    "else:\n",
    "    epsilon = 1. #valor de epsilon\n",
    "epsilon_min = 0.01 #valor minimo de epsilon\n",
    "epsilon_decay = (epsilon - epsilon_min) / (epocas - epoca_init) #o valor que vai retirado do epsilon por epoca\n",
    "\n",
    "rewards = [0] #variavel para guardar rewards\n",
    "plotx = [0] #variavel para guardar valores a serem plotados do eixo x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3907.5\n",
      "1    3900.0\n",
      "Name: preco, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:635: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "####################### LEITURA DOS DADOS #######################################################\n",
    "dias = 220\n",
    "\n",
    "caminho_arquivo = ('./consolidado.csv')\n",
    "arquivo = pd.read_csv(caminho_arquivo) #le arquivo\n",
    "inputs = arquivo[index_arquivo]\n",
    "print(inputs['preco'][0:2])\n",
    "if versao_arquivo == 1: #se quiser usar apenas os dias com IND e ISP\n",
    "    inputs = inputs[inputs['IND'] != 0]\n",
    "    arquivo = arquivo[arquivo['IND'] != 0]\n",
    "\n",
    "dt = arquivo['dt'].values #cria coluna apenas dos dias\n",
    "\n",
    "steps = []\n",
    "ultimo_dia = 0\n",
    "dias_para_rodar = [] #variavel para colocar os dias a serem rodados\n",
    "j = 0\n",
    "hr = []\n",
    "h = 0\n",
    "\n",
    "for i in range( 0, len(dt) ):    \n",
    "    if (dt[i] != ultimo_dia):\n",
    "        steps.append(i) #numero de linhas entre dias\n",
    "        ultimo_dia = dt[i]\n",
    "        dias_para_rodar.append(j) #numero do dia\n",
    "        j += 1\n",
    "        h = 0\n",
    "    hr.append(h)\n",
    "    h += 1\n",
    "step_max = np.amax(hr)\n",
    "batch = step_max + 1\n",
    "\n",
    "#normalizacao dos dados\n",
    "hr = hr/step_max\n",
    "inputs['hr_int'] = hr\n",
    "pmean = np.mean( inputs.loc[:dias*batch, inputs.columns[0]] ) #define valor minimo do preco  #step = 106\n",
    "pstd = np.std( inputs.loc[:dias*batch, inputs.columns[0]] ) #define valor maximo do preco\n",
    "for i in range( inputs.shape[1] ): #roda normalizo para todas as colunas\n",
    "    imean = np.mean( inputs.loc[:dias*batch, inputs.columns[i]] ) #pega valor maximo\n",
    "    istd = np.std( inputs.loc[:dias*batch, inputs.columns[i]] ) #pega valor minimo\n",
    "    \n",
    "    inputs.loc[:, inputs.columns[i]] = ( inputs.loc[:, inputs.columns[i]] - imean ) / istd #normaliza prs\n",
    "    \n",
    "mini_batch_size = int(dias*batch*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 320)          960         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 320)          6720        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          82176       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          82176       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           dense_2[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 11)           5643        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 177,675\n",
      "Trainable params: 177,675\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "########################################   BIBLIOTECAS ####################################\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, concatenate\n",
    "from keras import optimizers\n",
    "from keras import backend as K            #importa backend para clear_session()\n",
    "\n",
    "\n",
    "##################### MODELO DQN ####################################################\n",
    "class DQNAgent:\n",
    "    ########################### INICIALIZA ###########################################\n",
    "    def __init__(self, state_size, mkt_size, action_size, epsilon, janela, n_neuronios, n_variaveis):\n",
    "        self.state_size = state_size\n",
    "        self.mkt_size = mkt_size\n",
    "        self.n_neuronios = n_neuronios\n",
    "        self.action_size = action_size\n",
    "        self.n_variaveis = n_variaveis\n",
    "        self.limpa_memoria_dia()\n",
    "        self.limpa_memoria_epoca()\n",
    "        self.gamma = 0.97       # discount rate\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.learning_rate = 1e-4\n",
    "        self.model = self.cria_modelo()\n",
    "        self.model.summary()\n",
    "        self.state = []\n",
    "        self.next_state = []\n",
    "        self.janela_precos = []\n",
    "        self.prox_janela_precos = []\n",
    "\n",
    "################################# REDE NEURAL ###########################################\n",
    "    def cria_modelo(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        #inputs\n",
    "        x1 = Input(shape=(self.state_size,))\n",
    "        x2 = Input(shape=(self.mkt_size,))\n",
    "        \n",
    "        #state hidden layers\n",
    "        a1 = Dense(320)(x1)\n",
    "        a2 = Dense(256)(a1)\n",
    "        \n",
    "        #mkt hidden layers\n",
    "        b1 = Dense(320)(x2)\n",
    "        b2 = Dense(256)(b1)\n",
    "        \n",
    "        #concatenacao\n",
    "        x12 = concatenate([a2, b2], axis=-1)\n",
    "        \n",
    "        #camada de processamento ????\n",
    "        \n",
    "        #camada de saida\n",
    "        y = Dense(self.action_size, activation='linear')(x12)\n",
    "        \n",
    "        model = Model(inputs=[x1, x2], outputs=y)\n",
    "        \n",
    "        sgd = optimizers.SGD(lr=self.learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "        return model\n",
    "\n",
    "    def limpa_memoria_dia(self):\n",
    "        self.janela_precos = np.zeros(self.mkt_size)\n",
    "        self.prox_janela_precos = np.zeros(self.mkt_size)\n",
    "        self.state = []\n",
    "        self.next_state = []\n",
    "    \n",
    "    def limpa_memoria_epoca(self):\n",
    "        self.memory = []\n",
    "        \n",
    "    def toma_acao(self):\n",
    "        if np.random.rand() <= self.epsilon: #se o numero aleatorio for menor que o epsilon\n",
    "            return random.randrange(self.action_size) #retorna acao aleatoria     \n",
    "        act_values = self.model.predict([self.state, self.janela_precos]) #calcula qual a melhor acao\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "    \n",
    "    def treina_modelo(self, batch_size=mini_batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        x1 = np.zeros((batch_size+1, self.state_size))\n",
    "        x2 = np.zeros((batch_size+1, self.mkt_size))\n",
    "        y = np.zeros((batch_size+1, self.action_size))\n",
    "        i = 0\n",
    "        for acao, reward, estado, mercado, prox_estado, prox_mercado, done in minibatch:        \n",
    "            \n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict([prox_estado, prox_mercado])[0])) #pega valor que quer chegar\n",
    "\n",
    "            target_f = self.model.predict([estado, mercado]) #pega valor que chegou\n",
    "            target_f[0][acao+lim_cont] = target #define o valor que deseja chegar\n",
    "            x1[i,:] = estado\n",
    "            x2[i,:] = mercado\n",
    "            y[i,:] = target_f\n",
    "            i = i + 1\n",
    "            \n",
    "        self.model.fit([x1, x2], y, epochs=1, verbose=0) #treina modelo\n",
    "            \n",
    "    def remember(self, acao, reward, done):\n",
    "        self.memory.append((acao, reward, self.state, self.janela_precos, self.next_state, self.prox_janela_precos, done))\n",
    "    \n",
    "    def carrega_pesos(self, name):\n",
    "        self.model.load_weights(name) #carrega pesos\n",
    "\n",
    "    def salva_pesos(self, name):\n",
    "        self.model.save_weights(name) #salva pesos\n",
    "                \n",
    "########################  DECLARA MODELO ################################\n",
    "modelo = DQNAgent(n_estados, n_mercado, n_saidas, epsilon, janela, n_neuronios, n_variaveis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### FUNCOES ###############################################################\n",
    "\n",
    "def atuacao( preco, ncont, acao, custo, valor ):  #preo atual, n de contratos posicionados,\n",
    "                                                #acaoo atual, custo, valor da posio\n",
    "    preco_cheio = 0.\n",
    "    valor_cheio = 0.\n",
    "    ncont_anterior = ncont #salva posio anterior\n",
    "    ncont = acao #posicao atual = acao\n",
    "    reward = 0.\n",
    "    posicao = 0.\n",
    "    dp = 0.\n",
    "\n",
    "    \n",
    "    #variaveis para tratamento do sinal de ncont, ncont_anterior e da acao\n",
    "    acao = ncont - ncont_anterior  #ajuda a manter as condições anteriores\n",
    "    var1 = ncont*ncont_anterior\n",
    "    var2 = ncont*acao\n",
    "\n",
    "    if acao != 0:\n",
    "        #realizacao de lucros e custos\n",
    "        if ((var2>0) and (var1>=0)):\n",
    "            #aumento da posicao\n",
    "            reward = -custo*abs(acao)    #reward = -custo*acao se houve operacao (aumento da posicao)\n",
    "            #aumento da posicao = preco medio:\n",
    "            valor = (ncont_anterior*valor + acao*preco)/ncont  \n",
    "            \n",
    "        else:\n",
    "            valor_cheio = valor * pstd + pmean  #valores nao normalizados\n",
    "            preco_cheio = preco * pstd + pmean\n",
    "            if (var1)>= 0:\n",
    "                #diminuicao da posicao\n",
    "                reward = acao*(valor_cheio - preco_cheio)*10 - custo*abs(acao)  #reward se houve diminuicao da posicao\n",
    "                #valor medio nao muda na diminuicao, somente se ncont==0:\n",
    "                if ncont==0: valor=-10\n",
    "                    \n",
    "            else:\n",
    "                #troca de posicao\n",
    "                reward = (-ncont_anterior)*(valor_cheio - preco_cheio)*10 - custo*abs(acao)  #reward se houve troca da posicao\n",
    "                #preco medio da posicao = preco da troca de posicao\n",
    "                valor = preco\n",
    "    \n",
    "    #definindo o lucro potencial da carteira(posicao)\n",
    "    if valor!=-10:\n",
    "        valor_cheio = valor * pstd + pmean  #valor posicionado atual      \n",
    "        dp = (preco * pstd + pmean) - valor_cheio #variacao do preco atual e do preco de compra/venda\n",
    "        posicao = ncont * dp * 10 - custo*abs(ncont)           #posicao = lucro (POTENCIAL)\n",
    "        \n",
    "    return ncont, valor, posicao, reward\n",
    "\n",
    "def obter_acao(ncont):\n",
    "    decisao = modelo.toma_acao() #calcula a saida da rede neural\n",
    "    return (decisao - lim_cont)\n",
    "\n",
    "def rodar_1dia(precos, custo, dia):\n",
    "    global melhor_reward\n",
    "    ncont = 0 #cria variavel de quantidade de contratos\n",
    "    ncont_anterior = 0 #cria variavel para quantidade de contratos anterior\n",
    "    valor = -10 #cria variavel para preo medio\n",
    "    reward = 0. #cria variavel para recompensa\n",
    "    lucro = 0.\n",
    "    posicao = 0 #cria variavel de posio \n",
    "    erro = []\n",
    "    modelo.limpa_memoria_dia() #limpa o vetor de memoria\n",
    "    done = False\n",
    "    posicao_max = 10e4\n",
    "    shp = modelo.janela_precos.shape\n",
    "    \n",
    "    for step in range( steps[dia - 1], steps[dia] ):  #roda os dados\n",
    "        \n",
    "        ultimos_precos = precos[ step : step + 1 ] #pega os valores de agora\n",
    "        modelo.janela_precos = np.reshape(np.delete(np.insert(modelo.janela_precos,shp,np.concatenate(ultimos_precos.values).tolist()),[0,l]), (1,modelo.mkt_size))\n",
    "        modelo.state = np.reshape([ncont/lim_cont, valor], (1,modelo.state_size)) #adiciona na variavel de estado      \n",
    "        \n",
    "        acao = obter_acao(ncont)        \n",
    "        ncont, valor, posicao, reward = atuacao(precos['preco'][step], ncont, acao, custo, valor)\n",
    "        \n",
    "        #pos acao\n",
    "        lucro += reward    #LUCRO ACUMULADO\n",
    "        \n",
    "        #v=valor if valor==-10 else valor*pstd+pmean\n",
    "        #p=precos['preco'][step]*pstd+pmean\n",
    "        #print(\"acao={0}; ncont={1}; valor={2}; preco={3}; posicao={4}; lucro={5}; reward={6}\".format(acao, ncont, v, p, posicao, lucro, reward))   \n",
    "\n",
    "        if step == (steps[dia] - 1): #se ultimo step do dia\n",
    "            done = True     \n",
    "            prox_precos = np.zeros(n_variaveis)\n",
    "            modelo.prox_janela_precos = np.reshape(np.delete(np.insert(modelo.prox_janela_precos,shp,prox_precos),[0,l]),(1,modelo.mkt_size))\n",
    "        else:\n",
    "            prox_precos = precos[ step + 1 : step + 2 ] #pega os proximos valores\n",
    "            modelo.prox_janela_precos = np.reshape(np.delete(np.insert(modelo.prox_janela_precos,shp,np.concatenate(prox_precos.values).tolist()),[0,l]),(1,modelo.mkt_size))\n",
    "        \n",
    "        modelo.next_state = np.reshape([ncont/lim_cont, valor], (1,modelo.state_size)) #adiciona na variavel de estado      \n",
    "        modelo.remember(acao, posicao, done)  #salva step na memoria - reward = lucro instantaneo que a ação gerou\n",
    "        \n",
    "    #fim do dia\n",
    "    lucro += posicao\n",
    "\n",
    "    #print(\"ncont zerados={0}; posicao={1}\".format(ncont, posicao))\n",
    "    if lucro > melhor_reward:\n",
    "        melhor_reward = lucro\n",
    "    return lucro #retorna o valor do reward\n",
    "\n",
    "dias_pos = 0\n",
    "dias_neg = 0\n",
    "\n",
    "def rodar_dias(precos, custo):   \n",
    "    global dias_pos\n",
    "    global dias_neg\n",
    "    sum_rewards = 0 #cria variavel de somatoria de recompensas\n",
    "    \n",
    "    for dia in range( 1, dias ): #loop de dias\n",
    "        reward = rodar_1dia(precos, custo, dia)\n",
    "        sum_rewards += reward #roda 1 dia e adiciona o total na variavel de somatoria\n",
    "        \n",
    "        if reward>0: \n",
    "            dias_pos += 1\n",
    "        elif reward<0:\n",
    "            dias_neg += 1\n",
    "    return sum_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1105 22:24:41.361346  1664 deprecation_wrapper.py:119] From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resultado da epoca 0 = -38389.68 Epsilon = 1.000\n",
      "93\n",
      "126\n",
      "resultado da epoca 1 = -41805.58 Epsilon = 0.999\n",
      "88\n",
      "131\n",
      "25\n",
      "31\n",
      "Melhor resultado diario: 3051.18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-299593051ee8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoca\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoca_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepocas\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#rodar uma quantidade de epocas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mmodelo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimpa_memoria_epoca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0msum_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrodar_dias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcusto\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#adiciona o resultado da epoca na somatoria\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mmodelo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtreina_modelo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#roda o modelo com toda a memoria da epoca\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0msum_rewards_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msum_rewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-b7bf6b1dbd8d>\u001b[0m in \u001b[0;36mrodar_dias\u001b[1;34m(precos, custo)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdia\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdias\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#loop de dias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrodar_1dia\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcusto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdia\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0msum_rewards\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;31m#roda 1 dia e adiciona o total na variavel de somatoria\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-b7bf6b1dbd8d>\u001b[0m in \u001b[0;36mrodar_1dia\u001b[1;34m(precos, custo, dia)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mmodelo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mncont\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlim_cont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodelo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#adiciona na variavel de estado\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0macao\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobter_acao\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mncont\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mncont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposicao\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0matuacao\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'preco'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macao\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcusto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-b7bf6b1dbd8d>\u001b[0m in \u001b[0;36mobter_acao\u001b[1;34m(ncont)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mobter_acao\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mncont\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mdecisao\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoma_acao\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#calcula a saida da rede neural\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdecisao\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlim_cont\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-d4afcc44618e>\u001b[0m in \u001b[0;36mtoma_acao\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#se o numero aleatorio for menor que o epsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#retorna acao aleatoria\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mact_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjanela_precos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#calcula qual a melhor acao\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# returns action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1462\u001b[1;33m                                             callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "##################################  MAIN  #########################################\n",
    "if __name__ == \"__main__\":\n",
    "    global dias_pos\n",
    "    global dias_neg\n",
    "    sum_rewards_total = 0\n",
    "    sr=[]\n",
    "    try:\n",
    "        if carregar_pesos:\n",
    "            modelo.carrega_pesos('./pesos.h5')\n",
    "        epoca_parou = epoca_init\n",
    "        for epoca in range(epoca_init, epocas+5): #rodar uma quantidade de epocas\n",
    "            modelo.limpa_memoria_epoca()\n",
    "            sum_rewards = rodar_dias(inputs, custo) #adiciona o resultado da epoca na somatoria\n",
    "            modelo.treina_modelo() #roda o modelo com toda a memoria da epoca\n",
    "            sum_rewards_total += sum_rewards\n",
    "            sr.append(sum_rewards)\n",
    "            print(\"resultado da epoca {0} = {1:0.2f} Epsilon = {2:0.3f}\".format(epoca, sum_rewards, modelo.epsilon))\n",
    "            print(dias_pos)\n",
    "            print(dias_neg)\n",
    "            epoca_parou += 1\n",
    "            if ((epoca % 200) == 0):\n",
    "                modelo.salva_pesos('./pesos_5.h5')\n",
    "            dias_pos = 0\n",
    "            dias_neg = 0            \n",
    "            if modelo.epsilon <= epsilon_min:\n",
    "                modelo.epsilon = epsilon_min\n",
    "            else:\n",
    "                modelo.epsilon -= epsilon_decay\n",
    "    finally:\n",
    "        print(dias_pos)\n",
    "        print(dias_neg)\n",
    "        modelo.salva_pesos('./pesos_5.h5')\n",
    "        if carregar_epoca_epsilon:\n",
    "            file = open(\"./epoca_epsilon.txt\", \"w\")\n",
    "            file.writelines(\"{0},{1}\".format(epoca_parou, modelo.epsilon))\n",
    "            file.close()\n",
    "            print(\"parou na epoca {0} com epsilon {1}\".format(epoca_parou, modelo.epsilon))\n",
    "            \n",
    "        print(\"Melhor resultado diario: {0:0.2f}\".format(melhor_reward))\n",
    "        plt.plot(range(0, len(sr)), sr, range(0, len(sr)), np.zeros(len(sr))) #plota os valores de reward por epoca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
